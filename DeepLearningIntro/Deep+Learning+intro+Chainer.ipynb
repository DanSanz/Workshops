{"nbformat_minor": 0, "cells": [{"execution_count": 31, "cell_type": "code", "source": "from azureml import Workspace\n\nws = Workspace()\nds = ws.datasets['MNIST Train 60k 28x28 dense']\nds2 = ws.datasets['MNIST Test 10k 28x28 dense']\ntrain = ds.to_dataframe().values\ntest = ds2.to_dataframe().values", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 80, "cell_type": "code", "source": "try:\n    import chainer\nexcept ImportError, e:\n    !pip install chainer\n    import chainer\n    \nimport os\nimport six\nimport numpy as np\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import serializers\nfrom chainer import optimizers", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 65, "cell_type": "code", "source": "data_train = train[:,1:].astype(np.float32)\ndata_test = test[:,1:].astype(np.float32)\nlabels_train = train[:,0].astype(np.int32)\nlabels_test = test[:,0].astype(np.int32)\n\ndata_train /=255\ndata_test /=255\n\nN = labels_train.size\nN_test = labels_test.size\n\ndata_train = data_train.reshape(N,1,28,28)\ndata_test = data_test.reshape(N_test,1,28,28)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Start defining Neural Network architectures", "cell_type": "markdown", "metadata": {}}, {"source": "One Hidden Layer Network", "cell_type": "markdown", "metadata": {}}, {"execution_count": 101, "cell_type": "code", "source": "nb_nodes = 200\n\nclass OneLayer(chainer.Chain):\n    def __init__(self):\n        super(OneLayer, self).__init__(\n            l1=L.Linear(None,nb_nodes),\n            l2=L.Linear(nb_nodes,10)            \n        )\n    def __call__(self,x):\n        h1 =F.sigmoid(self.l1(x))\n        y =self.l2(h1)\n        return y", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Two Hidden Layers Network", "cell_type": "markdown", "metadata": {}}, {"execution_count": 102, "cell_type": "code", "source": "class TwoLayer(chainer.Chain):\n    def __init__(self):\n        super(TwoLayer, self).__init__(\n            l1=L.Linear(None,nb_nodes),\n            l2=L.Linear(nb_nodes,nb_nodes),\n            l3=L.Linear(nb_nodes,10)\n        )\n    def __call__(self,x):\n        h1 =F.sigmoid(self.l1(x))\n        h2 = F.sigmoid(self.l2(h1))\n        y =self.l3(h2)\n        return y", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Basic Convolution", "cell_type": "markdown", "metadata": {}}, {"execution_count": 126, "cell_type": "code", "source": "class SimpleCNN(chainer.Chain):\n    def __init__(self):\n        super(SimpleCNN, self).__init__(\n            c1=L.Convolution2D(1, 5, 5),\n            c2=L.Convolution2D(5, 10, 5),\n            l3=L.Linear(4000, 100),\n            l4=L.Linear(100,10)\n        )\n\n    def __call__(self, x):\n        nl1 = self.c1(x)\n        nl2 = self.c2(nl1)\n        nl3 = F.sigmoid(nl2)\n        nl4 = self.l3(nl3)\n        nl5 = F.sigmoid(nl4)\n        y = self.l4(nl5)\n        return y", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Convolution and Pooling", "cell_type": "markdown", "metadata": {}}, {"execution_count": 145, "cell_type": "code", "source": "class PoolingCNN(chainer.Chain):\n    def __init__(self):\n        super(PoolingCNN,self).__init__(\n            c1=L.Convolution2D(1, 5, 5),\n            c2=L.Convolution2D(5, 10, 5),\n            l3=L.Linear(160, 100),\n            l4=L.Linear(100,10)\n        )\n    \n    def __call__(self, x):\n        nl1 = self.c1(x)\n        nl2 = F.max_pooling_2d(nl1, 2, stride=2)\n        nl3 = self.c2(nl2)\n        nl4 = F.max_pooling_2d(nl3, 2, stride=2)\n        nl5 = F.sigmoid(nl4)\n        nl6 = self.l3(nl5)\n        nl7 = F.sigmoid(nl6)\n        y = self.l4(nl7)\n        return y\n    ", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Train model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 105, "cell_type": "code", "source": "def train_model(model, batchsize = 100, num_epochs = 20):\n    \"\"\"\n    Trains a Chainer model.\n    \n    Arguments:\n    \n    model -- The model to train\n    \n    Keyword arguments:\n    \n    batchsize -- The batchsize to use during training\n    epoch -- The number of epochs to train\n    \"\"\"\n    \n    optimizer = optimizers.Adam()\n    optimizer.setup(model)\n    accuracy = 0\n\n    for epoch in six.moves.range(1, num_epochs + 1):\n        print('epoch', epoch)\n\n        # training\n        perm = np.random.permutation(N)\n        sum_accuracy = 0\n        sum_loss = 0\n        \n        for i in six.moves.range(0, N, batchsize):\n            x = chainer.Variable(np.asarray(data_train[perm[i:i + batchsize]]))\n            t = chainer.Variable(np.asarray(labels_train[perm[i:i + batchsize]]))\n\n            #run the forward pass, compute the loss, do backprop and then update the weights\n            optimizer.update(model, x, t)\n\n            sum_loss += float(model.loss.data) * len(t.data)\n            sum_accuracy += float(model.accuracy.data) * len(t.data)\n        \n        print('train mean loss={}, accuracy={}'.format(sum_loss / N, sum_accuracy / N))\n\n        # evaluation\n        sum_accuracy = 0\n        sum_loss = 0\n        for i in six.moves.range(0, N_test, batchsize):\n            x = chainer.Variable(np.asarray(data_test[i:i + batchsize]),\n                                 volatile='on')\n            t = chainer.Variable(np.asarray(labels_test[i:i + batchsize]),\n                                 volatile='on')\n            loss = model(x, t)\n            sum_loss += float(loss.data) * len(t.data)\n            sum_accuracy += float(model.accuracy.data) * len(t.data)\n\n        print('test  mean loss={}, accuracy={}'.format(sum_loss / N_test, sum_accuracy / N_test))\n        accuracy = sum_accuracy / N_test\n    return accuracy", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 146, "cell_type": "code", "source": "onelayer_model = L.Classifier(OneLayer(),lossfun=F.softmax_cross_entropy)\ntwolayer_model = L.Classifier(TwoLayer(),lossfun=F.softmax_cross_entropy)\nconv_model = L.Classifier(SimpleCNN(), lossfun=F.softmax_cross_entropy)\npool_model = L.Classifier(PoolingCNN(), lossfun=F.softmax_cross_entropy)", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "Train and evalute one hidden layer network", "cell_type": "markdown", "metadata": {}}, {"execution_count": 108, "cell_type": "code", "source": "accuracy = np.zeros(4)\naccuracy[0] = train_model(onelayer_model, num_epochs = 20)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "('epoch', 1)\ntrain mean loss=0.0138009185843, accuracy=0.99763333559\ntest  mean loss=0.0667272699838, accuracy=0.979000006318\n('epoch', 2)\ntrain mean loss=0.0109935877408, accuracy=0.998666667938\ntest  mean loss=0.0665229406033, accuracy=0.979800004959\n('epoch', 3)\ntrain mean loss=0.00923426642296, accuracy=0.999116667509\ntest  mean loss=0.0679031881661, accuracy=0.979500006437\n('epoch', 4)\ntrain mean loss=0.00790773673488, accuracy=0.999333333969\ntest  mean loss=0.0669504509203, accuracy=0.979800004959\n('epoch', 5)\ntrain mean loss=0.00686494637281, accuracy=0.999366667271\ntest  mean loss=0.0675697085301, accuracy=0.979800006747\n('epoch', 6)\ntrain mean loss=0.00581161586975, accuracy=0.99956666708\ntest  mean loss=0.0684038205327, accuracy=0.979800004363\n('epoch', 7)\ntrain mean loss=0.00487792049826, accuracy=0.999733333588\ntest  mean loss=0.0680542972581, accuracy=0.979500004053\n('epoch', 8)\ntrain mean loss=0.00420592786594, accuracy=0.999816666842\ntest  mean loss=0.0687279174062, accuracy=0.980000004172\n('epoch', 9)\ntrain mean loss=0.00348480661036, accuracy=0.999900000095\ntest  mean loss=0.0694795525463, accuracy=0.979900004864\n('epoch', 10)\ntrain mean loss=0.00307762372647, accuracy=0.999866666794\ntest  mean loss=0.0711675249279, accuracy=0.979700005054\n('epoch', 11)\ntrain mean loss=0.00235343791976, accuracy=1.0\ntest  mean loss=0.0700823107899, accuracy=0.980200003982\n('epoch', 12)\ntrain mean loss=0.00217514273131, accuracy=0.999916666746\ntest  mean loss=0.0728220642837, accuracy=0.979300005436\n('epoch', 13)\ntrain mean loss=0.00185956812236, accuracy=0.999950000048\ntest  mean loss=0.072605393591, accuracy=0.980500007272\n('epoch', 14)\ntrain mean loss=0.00168608383174, accuracy=0.999950000048\ntest  mean loss=0.0737169248356, accuracy=0.981100004315\n('epoch', 15)\ntrain mean loss=0.00123547483272, accuracy=1.0\ntest  mean loss=0.0744137988983, accuracy=0.98020000577\n('epoch', 16)\ntrain mean loss=0.00105740572436, accuracy=1.0\ntest  mean loss=0.077017718137, accuracy=0.979600005746\n('epoch', 17)\ntrain mean loss=0.00089054037504, accuracy=1.0\ntest  mean loss=0.0779125787605, accuracy=0.979600005746\n('epoch', 18)\ntrain mean loss=0.0010017741271, accuracy=0.999950000048\ntest  mean loss=0.085639797903, accuracy=0.978800006509\n('epoch', 19)\ntrain mean loss=0.00126781552259, accuracy=0.999933333397\ntest  mean loss=0.0812592148958, accuracy=0.98020000577\n('epoch', 20)\ntrain mean loss=0.00052564397795, accuracy=1.0\ntest  mean loss=0.0824718600247, accuracy=0.980000007153\n"}], "metadata": {"collapsed": false}}, {"source": "Train and evaluate Two hidden layers network", "cell_type": "markdown", "metadata": {}}, {"execution_count": 109, "cell_type": "code", "source": "accuracy[1] = train_model(twolayer_model, num_epochs = 20)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "('epoch', 1)\ntrain mean loss=0.00479928974365, accuracy=0.998800001144\ntest  mean loss=0.0853722799084, accuracy=0.98060000658\n('epoch', 2)\ntrain mean loss=0.00242555570214, accuracy=0.999650000334\ntest  mean loss=0.0849670733628, accuracy=0.979700006247\n('epoch', 3)\ntrain mean loss=0.00534159680089, accuracy=0.99848333478\ntest  mean loss=0.0922220753541, accuracy=0.979400007129\n('epoch', 4)\ntrain mean loss=0.00262836227231, accuracy=0.999416667223\ntest  mean loss=0.08630336076, accuracy=0.98050000608\n('epoch', 5)\ntrain mean loss=0.000738999309287, accuracy=1.0\ntest  mean loss=0.0858044913109, accuracy=0.981300007105\n('epoch', 6)\ntrain mean loss=0.00113235135634, accuracy=0.999800000191\ntest  mean loss=0.0895912206624, accuracy=0.980100006461\n('epoch', 7)\ntrain mean loss=0.0067389647451, accuracy=0.997733335495\ntest  mean loss=0.0932179689693, accuracy=0.979800006151\n('epoch', 8)\ntrain mean loss=0.00114720687534, accuracy=0.999816666842\ntest  mean loss=0.0870978020306, accuracy=0.982300006747\n('epoch', 9)\ntrain mean loss=0.000862168293653, accuracy=0.999800000191\ntest  mean loss=0.0878717531561, accuracy=0.982500006557\n('epoch', 10)\ntrain mean loss=0.000446170459534, accuracy=0.999933333397\ntest  mean loss=0.0897983350438, accuracy=0.982100006342\n('epoch', 11)\ntrain mean loss=0.000219229121349, accuracy=1.0\ntest  mean loss=0.0861142915397, accuracy=0.982000006437\n('epoch', 12)\ntrain mean loss=0.00997775748271, accuracy=0.996666669746\ntest  mean loss=0.08956794825, accuracy=0.980600007176\n('epoch', 13)\ntrain mean loss=0.00115899474545, accuracy=0.999766666889\ntest  mean loss=0.0892211373385, accuracy=0.980700005889\n('epoch', 14)\ntrain mean loss=0.000246836091977, accuracy=1.0\ntest  mean loss=0.0881891799978, accuracy=0.981800007224\n('epoch', 15)\ntrain mean loss=0.000154129150266, accuracy=1.0\ntest  mean loss=0.0878075715927, accuracy=0.981600006223\n('epoch', 16)\ntrain mean loss=0.000127433777933, accuracy=1.0\ntest  mean loss=0.0890864062234, accuracy=0.981500006914\n('epoch', 17)\ntrain mean loss=0.000127338094572, accuracy=1.0\ntest  mean loss=0.0888249360098, accuracy=0.98140000701\n('epoch', 18)\ntrain mean loss=0.000102860275457, accuracy=1.0\ntest  mean loss=0.0911986564652, accuracy=0.982400006652\n('epoch', 19)\ntrain mean loss=8.3311246182e-05, accuracy=1.0\ntest  mean loss=0.0927958852959, accuracy=0.982100007534\n('epoch', 20)\ntrain mean loss=6.84531872518e-05, accuracy=1.0\ntest  mean loss=0.093095538131, accuracy=0.982600005865\n"}], "metadata": {"collapsed": false}}, {"source": "Train and evaluate Basic Convolution Network", "cell_type": "markdown", "metadata": {}}, {"execution_count": 128, "cell_type": "code", "source": "accuracy[2] = train_model(conv_model, num_epochs = 20)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "('epoch', 1)\ntrain mean loss=0.458803639207, accuracy=0.888016668359\ntest  mean loss=0.126975066625, accuracy=0.96650000453\n('epoch', 2)\ntrain mean loss=0.101595068521, accuracy=0.972650008202\ntest  mean loss=0.0735563258268, accuracy=0.979200006723\n('epoch', 3)\ntrain mean loss=0.0652188406714, accuracy=0.982350010773\ntest  mean loss=0.0549358542939, accuracy=0.982400007844\n('epoch', 4)\ntrain mean loss=0.0478772817296, accuracy=0.986633343498\ntest  mean loss=0.0492632593343, accuracy=0.984800006747\n('epoch', 5)\ntrain mean loss=0.0376000160693, accuracy=0.989800008237\ntest  mean loss=0.0475258720492, accuracy=0.984900007248\n('epoch', 6)\ntrain mean loss=0.0300541626856, accuracy=0.99215000699\ntest  mean loss=0.0424096124654, accuracy=0.986300008297\n('epoch', 7)\ntrain mean loss=0.0234046762064, accuracy=0.993650005758\ntest  mean loss=0.0393977614006, accuracy=0.987200004458\n('epoch', 8)\ntrain mean loss=0.0186166798372, accuracy=0.995316671133\ntest  mean loss=0.039381391533, accuracy=0.986300007105\n('epoch', 9)\ntrain mean loss=0.0147827949499, accuracy=0.996583336492\ntest  mean loss=0.0408128192078, accuracy=0.986800006628\n('epoch', 10)\ntrain mean loss=0.0107447571513, accuracy=0.997783335447\ntest  mean loss=0.0481694382141, accuracy=0.984200007915\n('epoch', 11)\ntrain mean loss=0.00889376975091, accuracy=0.99828333497\ntest  mean loss=0.0353921708214, accuracy=0.988800004721\n('epoch', 12)\ntrain mean loss=0.00725738379629, accuracy=0.998550001383\ntest  mean loss=0.0461895739088, accuracy=0.98460000813\n('epoch', 13)\ntrain mean loss=0.00597040063144, accuracy=0.998900001049\ntest  mean loss=0.0386361881839, accuracy=0.988300005794\n('epoch', 14)\ntrain mean loss=0.00521642348676, accuracy=0.998966667652\ntest  mean loss=0.0400712708585, accuracy=0.98780000627\n('epoch', 15)\ntrain mean loss=0.00316807338548, accuracy=0.999600000381\ntest  mean loss=0.0381492827292, accuracy=0.988600005507\n('epoch', 16)\ntrain mean loss=0.00173207471479, accuracy=0.999883333445\ntest  mean loss=0.0367788610314, accuracy=0.989500006437\n('epoch', 17)\ntrain mean loss=0.00118742464917, accuracy=0.999950000048\ntest  mean loss=0.0374107259518, accuracy=0.989300006032\n('epoch', 18)\ntrain mean loss=0.000971587713205, accuracy=0.999966666698\ntest  mean loss=0.0384294558671, accuracy=0.989800005555\n('epoch', 19)\ntrain mean loss=0.0122968657469, accuracy=0.996283336778\ntest  mean loss=0.0468530164901, accuracy=0.985200008154\n('epoch', 20)\ntrain mean loss=0.00639405414816, accuracy=0.998516668081\ntest  mean loss=0.0408409753607, accuracy=0.987900006175\n"}], "metadata": {"collapsed": false}}, {"source": "Train and evaluate Convolution and Pooling Network", "cell_type": "markdown", "metadata": {}}, {"execution_count": 147, "cell_type": "code", "source": "accuracy[3] = train_model(pool_model, num_epochs = 20)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "('epoch', 1)\ntrain mean loss=0.847008854498, accuracy=0.777050000255\ntest  mean loss=0.258624150343, accuracy=0.932300003767\n('epoch', 2)\ntrain mean loss=0.202863755325, accuracy=0.944400002162\ntest  mean loss=0.142996984748, accuracy=0.95740000248\n('epoch', 3)\ntrain mean loss=0.131877895265, accuracy=0.96226667136\ntest  mean loss=0.107767989729, accuracy=0.969200006723\n('epoch', 4)\ntrain mean loss=0.101724174898, accuracy=0.969800008138\ntest  mean loss=0.0852272914047, accuracy=0.974100006223\n('epoch', 5)\ntrain mean loss=0.0857837808629, accuracy=0.974366675417\ntest  mean loss=0.0770316862804, accuracy=0.976600006223\n('epoch', 6)\ntrain mean loss=0.0750181726723, accuracy=0.977400009632\ntest  mean loss=0.0642613154696, accuracy=0.980500005484\n('epoch', 7)\ntrain mean loss=0.0667218869211, accuracy=0.98025001049\ntest  mean loss=0.0614543408761, accuracy=0.980100007653\n('epoch', 8)\ntrain mean loss=0.0610540196641, accuracy=0.981350010633\ntest  mean loss=0.0582931590476, accuracy=0.981200006604\n('epoch', 9)\ntrain mean loss=0.0558815965833, accuracy=0.983400010864\ntest  mean loss=0.0557843092008, accuracy=0.981900007725\n('epoch', 10)\ntrain mean loss=0.0515650642826, accuracy=0.984383344054\ntest  mean loss=0.0506360388378, accuracy=0.983700006604\n('epoch', 11)\ntrain mean loss=0.0486619314148, accuracy=0.985533343256\ntest  mean loss=0.0516133847646, accuracy=0.982800006866\n('epoch', 12)\ntrain mean loss=0.0451530032163, accuracy=0.986200010081\ntest  mean loss=0.050495523033, accuracy=0.984000008106\n('epoch', 13)\ntrain mean loss=0.0423286890696, accuracy=0.986966676613\ntest  mean loss=0.047861594719, accuracy=0.984200006723\n('epoch', 14)\ntrain mean loss=0.0400264097312, accuracy=0.987666676541\ntest  mean loss=0.0473757538837, accuracy=0.983900006413\n('epoch', 15)\ntrain mean loss=0.0379158490361, accuracy=0.988900009096\ntest  mean loss=0.0450428285685, accuracy=0.98460000515\n('epoch', 16)\ntrain mean loss=0.0365622983496, accuracy=0.988666675985\ntest  mean loss=0.0428863685025, accuracy=0.985700007677\n('epoch', 17)\ntrain mean loss=0.0345396585394, accuracy=0.989516675572\ntest  mean loss=0.0440287639064, accuracy=0.984900006056\n('epoch', 18)\ntrain mean loss=0.0328263832914, accuracy=0.99016667515\ntest  mean loss=0.041892918208, accuracy=0.985700007677\n('epoch', 19)\ntrain mean loss=0.0307839323988, accuracy=0.990933340689\ntest  mean loss=0.0433251880162, accuracy=0.985500007868\n('epoch', 20)\ntrain mean loss=0.0300736982156, accuracy=0.991150007943\ntest  mean loss=0.042282113949, accuracy=0.986000007391\n"}], "metadata": {"collapsed": false}}, {"execution_count": 148, "cell_type": "code", "source": "accuracy", "outputs": [{"execution_count": 148, "output_type": "execute_result", "data": {"text/plain": "array([ 0.98000001,  0.98260001,  0.98790001,  0.98600001])"}, "metadata": {}}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.12", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}